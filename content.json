[{"title":"HBase部署方式","date":"2017-03-31T05:48:08.000Z","path":"2017/03/31/2017-03-31-HBase-deploy-mode/","text":"Standalone Mode单节点独立部署 1. 下载并解压HBase安装包2. 在hbase-env.sh中配置JAVA_HOME3. 编辑hbase-site.xml，配置hbase.rootdir和hbase.zookeeper.property.dataDir12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:///home/testuser/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/testuser/zookeeper&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4. 启动123456789Standalone模式下，所有的HBase守护进程都运行在一个JVM中，像HMaster、一个HRegionServer，和ZooKeeper。访问http://localhost:16010 来查看HBase的Web UI## 伪分布式部署伪分布式意味着HBase仍旧运行在一台机器上，但是每一个HBase守护进程（HMaster、HRegionServer、ZooKeeper）都作为一个独立的进程运行，而Standalone模式下，所有守护进程都运行在一个JVM中。### 1. 配置hbase-site.xml,添加如下属性 hbase.cluster.distributed true 12### 2. 启动HBase```bin/start-hbase.sh 3. 如果配置的hbase.rootDir是在HDFS，检查HDFS上是否有对应的hbase数据目录4. 做一些数据库操作12345678create &apos;test&apos;, &apos;cf&apos;list &apos;test&apos;put &apos;test&apos;, &apos;row1&apos;, &apos;cf:a&apos;, &apos;value1&apos;put &apos;test&apos;, &apos;row2&apos;, &apos;cf:b&apos;, &apos;value2&apos;put &apos;test&apos;, &apos;row3&apos;, &apos;cf:c&apos;, &apos;value3&apos;scan &apos;test&apos;disable &apos;test&apos;drop &apos;test&apos; 5. 启动停止backup HMasterstart 2 3 5 ```1234567891011121314HMaster会占用三个端口（默认是16010， 16020， 16030）backup HMaster的端口偏移量在脚本后面作为参数最多一个机器上可以启动10个HMasterstop backup HMastercat /tmp/hbase-testuser-2-master.pid |xargs kill ### 6. 启动停止额外的HRegionServer同backup HMaster类似HRegionServer会占用两个端口（默认是16020， 16030，不过这两个端口已经被同一台机器上的HMaster占用，从HBase1.0.0开始，HMaster也是HRegionServer，所以这里的HRegionServer端口变为16200， 16300）一台机器上，最多可以启动100个HRegionServer```bin/local-regionservers.sh start 2 3 5 ``上面启动的HRegionServer端口针对16200/16300的偏移量分别是2、3、5停止HRegionServer```bin/local-regionservers.sh stop 2 7. 停止HBase1234567891011## 完全分布式（Fully Distributed）|Node Name|Master|RegionServer|ZooKeeper||-----|-----|-----|-----||node-a|yes|no|yes||node-b|backup|yes|yes||node-c|no|yes|yes|### 1. 配置无密码SSH登陆1. 在Master上生成ssh key ``` ssh-keygen -t rsa 将生成的id_rsa.pub的内容，放到~/.ssh/authorized_keys文件的末尾（如果没有，需要自行创建） 将Master上的公钥id_rsa.pub通过scp工具传到其他节点，并将内容添加到authorized_keys文件末尾 测试Master能否无密码登陆本机和其他节点 在backup HMaster节点做同样的操作，注意不要覆盖authorized_keys文件的已有内容2. 配置Master Master上运行HMaster和ZooKeeper，但是不运行HRegionServer将conf/regionservers里面的node-a的信息去掉，添加node-b、node-c的hostname 新建一个文件conf/backup-masters,内容填写node-b的hostname 配置ZooKeeper以下配置，让HBase在集群的每个节点上启动和管理一个ZooKeeper实例123456789&lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/Users/ligz/data/zookeeper&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt; &lt;description&gt;usually comma seperated hostname list&lt;/description&gt;&lt;/property&gt; 3. 准备node-b， node-c将node-a上的hbase安装目录，通过scp工具传到node-b和node-c上 4. 启动集群，并测试启动前确保各个节点上的HBase相关进程已经停止（HMaster、HRegionServer、HQuorum）在node-a上执行 启动集群，观察控制台输出。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162控制台顺序输出：启动ZooKeeper，启动Master，之后是RegionServer，最后是Backup Master登陆各个节点，通过jps查看是否有相应的进程### 5. 通过浏览器查看Web UIMaster访问端口16010（HBase0.98.x之前的版本，端口是60010）RegionServer访问端口16030（Hbase0.98.x之前的版本，端口是60030）## 部署模式正如上面介绍的，分为Standalone和Distributed两种### Standalone模式所有的HBase守护进程都运行在一个JVM中。默认情况下，Standalone模式使用本地文件系统进行持久化操作。如果想把数据持久化到HDFS上，需要修改hbase-site.xml中的两个配置项1. hbase.rootdir修改为HDFS路径hdfs://ip:port/hbase2. hbase.cluster.distributed设置为false### Distributed模式分为伪分布式和完全分布式，命名源自Hadoop里的伪分布式和分布式部署。伪分布式可以持久化到本地文件系统或者HDFS。完全分布式只支持HDFS。伪分布式是完全分布式模式的所有进程运行在一个节点上，仅仅用来在HBase上测试和建立原型。不要在生产环境使用这个模式，也不要用此模式对HBase进行性能评估完全分布式部署（Fully-Distributed），适用于生产环境。采用完全分布式部署，HBase的守护进程运行在集群的每个节点上。完全分布式部署需要修改配置项为***hbase.cluster.distributed***为true，***hbase.rootdir为HDFS路径***。***conf/regionservers***里面配置集群中的RegionServer列表，每行一个hostname。ZooKeeper，可以使用HBase自己管理的ZooKeeper集群（默认），也可以使用独立的ZooKeeper集群。配置项为hbase-env.sh中的***HBASE_MANAGES_ZK***,默认是true，使用独立ZooKeeper集群时，修改此配置项为false。使用HBase管理的ZooKeeper集群时，ZooKeeper的配置参数可以在hbase-site.xml中直接配置，配置参数前缀使用hbase.zookeeper.property。例如ZooKeeper的clientPort，在hbase-site.xml中配置项为hbase.zookeeper.property.clientPort最少也需要配置```hbase.zookeeper.quorum```，默认只是绑定到了localhost，在完全分布式环境下并不适合，因为client连接不到。***hbase.zookeeper.property.dataDir***最好明确配置为/tmp以外的目录。默认/tmp目录会被系统清理。ZooKeeper***节点数目***，最好配置为***奇数***。这样可以更好地容忍节点失败。4个节点，容许1个节点fail，5个节点容许2个节点fail。配置ZooKeeper服务器1G内存，可以的话，使用专属磁盘。对于负担比较重的HBase集群，在RegionServers（DataNodes和TaskTrackers）之外的机器上运行ZooKeeper服务。ZooKeeper版本，最好使用最新的稳定版本。从HBase1.0.0开始，需要3.4.x版本的ZK。ZooKeeper维护设置，确保设置了***data dir 的cleaner***，要不然可能隔几个月会出现一些“有意思”的问题如果ZooKeeper运行在数十万个日志的目录，zk会开始删除会话，这会导致Leader election不能完成。虽然Leader Election很少进行，但是在机器丢失或者发生hipcup时，还是会随机发生。使用独立ZooKeeper集群，hbase-env.sh中的***HBASE_MANAGES_ZK***设置为falsehbase-site.xml中，设置***hbase.zookeeper.quorum***和***hbase.zookeeper.property.clientPort***不想在HBase start/stop时start/stop ZooKeeper的话，可以单独执行ZooKeeper的start/stopbin/hbase-daemons.sh &#123;start,stop&#125; zookeeper同样需要HBASE_MANAGES_ZK设置为falseHadoop Client端配置如果修改了Hadoop急群众HDFS客户端的一些配置，需要使用下面三种方法中的一种来让HBase使用修改后的配置1. 在hbase-env.sh中，向HBASE_CLASSPATH中添加HADOOP_CONF_DIR。2. 向hbase的conf目录添加hdfs-site.xml的一份拷贝，或者更好地，使用软连3. 如果只是几项HDFS配置修改的话，直接在hbase-site.xml中添加这几项修改。举例来说，dfs.replication这个参数，如果修改为5，HBase不作处理的话，依旧是3.## 运行和确认安装确保HDFS、ZooKeeper已经成功运行。可以通过 ```hdfs dfs -put from to ```来测试HDFS是否正常工作正常情况下，HBase不需要使用MapReduce和Yarn守护进程，所以这些不需要启动。启动HBase命令，需要在HBASE_HOME目录下运行 bin/start-hbase.sh123456HBase应该已经启动，可以检查$HBASE_HOME/logs里面的日志。HBase提供了一个UI来展示重要的属性。默认是在Master节点的16010端口。HBase RegionServer默认监听16020端口，并且在16030端口提供了HTTP服务。HBase0.98之前的版本，HBase Master UI监听在60010，HBase RegionServers UI监听在60030一旦HBase启动成功，可以执行一些create 、list、put、get、scan、disable、drop等命令，来查看HBase是否正常工作。停止HBase命令 ```bin/stop-hbase.sh 停止操作需要等待片刻才能完成。如果集群由许多机器组成，会等待更长时间。如果正在执行分布式操作，一定要等待HBase完全关闭后再去停止Hadoop守护进程。","tags":[{"name":"HBase,Standalone Mode, Distributed Mode","slug":"HBase-Standalone-Mode-Distributed-Mode","permalink":"http://yoursite.com/tags/HBase-Standalone-Mode-Distributed-Mode/"}]},{"title":"CNN-in-NLP","date":"2016-12-07T03:49:35.000Z","path":"2016/12/07/2016-12-07-cnn-in-nlp/","text":"摘自WILDML UNDERSTANDING CONVOLUTIONAL NEURAL NETWORKS FOR NLP理解用于自然语言处理的卷积神经网络November 7, 2015When we hear about Convolutional Neural Network (CNNs), we typically think of Computer Vision.当我们谈到卷积神经网络（CNN）时，通常会想到计算机视觉（Computer Vision）。 CNNs were responsible for major breakthroughs in Image Classification and are the core of most Computer Vision systems today, from Facebook’s automated photo tagging to self-driving cars.CNN在图像分类领域带来了重大突破，它是今天大部分计算机视觉系统的核心。不管是Facebook的自动为图片打标签，还是自动驾驶汽车系统。 More recently we’ve also started to apply CNNs to problems in Natural Language Processing and gotten some interesting results.最近我们开始在自然语言处理领域应用CNN，得到了一些有意思的结果。 In this post I’ll try to summarize what CNNs are, and how they’re used in NLP.在这篇文章中，我将尝试总结CNN是什么，它们是如何在NLP领域被使用的。 The intuitions behind CNNs are somewhat easier to understand for the Computer Vision use case, so I’ll start there, and then slowly move towards NLP.CNN背后的直觉认识在计算机视觉场景下是比较容易理解的，我会从计算机视觉场景谈起，之后介绍在NLP方面的应用。 WHAT IS CONVOLUTION?The for me easiest way to understand a convolution is by thinking of it as a sliding window function applied to a matrix.我认为理解卷积最简单的方式，就是把它当做一个应用在矩阵上的滑动窗口函数 That’s a mouthful, but it becomes quite clear looking at a visualization:这事是嘴上说，但是通过看一个图像，可以变得非常清楚。Convolution with 3×3 Filter. Source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution Imagine that the matrix on the left represents an black and white image.将左侧的矩阵想象为一幅黑白图像的表示Each entry corresponds to one pixel, 0 for black and 1 for white (typically it’s between 0 and 255 for grayscale images).每个条目对应一个像素，0表示黑，1表示白（对于灰度图像，通常是0到255之间的值）The sliding window is called a kernel, filter, or feature detector.滑动窗口被称为：核、过滤器或者特征检测器Here we use a 3×3 filter, multiply its values element-wise with the original matrix, then sum them up.这里我们使用一个3×3的过滤器，将它的每个元素的值与原始矩阵的元素对应相乘，然后求和。To get the full convolution we do this for each element by sliding the filter over the whole matrix.为了得到全部的卷积，我们通过在整个矩阵上滑动过滤器对每个元素执行一遍这个操作You may be wondering wonder what you can actually do with this. Here are some intuitive examples.你可能会好奇，用得到的卷积可以做什么呢。下面列举几个直观的例子。AVERAGING EACH PIXEL WITH ITS NEIGHBORING VALUES BLURS AN IMAGE:对像素及其周围像素点求平均，可以模糊图像 TAKING THE DIFFERENCE BETWEEN A PIXEL AND ITS NEIGHBORS DETECTS EDGES:对像素及其周围点求异，可以检测图像边缘(To understand this one intuitively, think about what happens in parts of the image that are smooth, where a pixel color equals that of its neighbors: The additions cancel and the resulting value is 0, or black. If there’s a sharp edge in intensity, a transition from white to black for example, you get a large difference and a resulting white value) The GIMP manual has a few other examples.链接GIMP manual处有另外的一些例子 To understand more about how convolutions work I also recommend checking out Chris Olah’s post on the topic.为了更深一步理解卷积是如何工作的，我推荐阅读Chris Olah’s post on the topic WHAT ARE CONVOLUTIONAL NEURAL NETWORKS?什么是卷积神经网络？Now you know what convolutions are. But what about CNNs?现在你知道卷积是什么了，那么CNN又是什么呢？CNNs are basically just several layers of convolutions with nonlinear activation functions like ReLU or tanh applied to the results.CNN原理上就是多层的卷积，并且带有非线性激活函数（像ReLU或者tanh）应用到每个卷积的结果 In a traditional feedforward neural network we connect each input neuron to each output neuron in the next layer. That’s also called a fully connected layer, or affine layer.在传统的前馈神经网络中，我们连接每个输入神经元到下一层的每一个输出神经元。这也被称为全连接层，或者仿射层。 In CNNs we don’t do that. Instead, we use convolutions over the input layer to compute the output.在CNN中，我们不这样做。相反，我们使用输入层上的卷积来计算输出。 This results in local connections, where each region of the input is connected to a neuron in the output.这导致了局部连接，其中每个输入区域连接到输出中的一个神经元。 Each layer applies different filters, typically hundreds or thousands like the ones showed above, and combines their results.每层使用不同的过滤器（或者说滑动窗口函数），通常成百上千个，如上所示，把他们的结果结合起来。 There’s also something something called pooling (subsampling) layers, but I’ll get into that later.还有一些叫做池化层，之后会讲到。 During the training phase, a CNN automatically learns the values of its filters based on the task you want to perform.训练阶段，CNN根据你要执行的任务，自动学习过滤器的值 For example, in Image Classification a CNN may learn to detect edges from raw pixels in the first layer, then use the edges to detect simple shapes in the second layer, and then use these shapes to deter higher-level features, such as facial shapes in higher layers.例如，在图像分类中，一个CNN可以学习在第一层从原始像素中检测边缘，然后在第二层使用边缘来检测简单形状，之后在更高的层使用形状来生成高层次特征，例如面部形状。–The last layer is then a classifier that uses these high-level features.–最后一层是一个使用这些高层次特征的分类器–Convolutional Neural Network (Clarifai) There are two aspects of this computation worth paying attention to: Location Invariance and Compositionality.这个计算有两处值得注意：位置不变性 和 组成性 Let’s say you want to classify whether or not there’s an elephant in an image.假定你想要分类图片中是否有大象Because you are sliding your filters over the whole image you don’t really care where the elephant occurs.因为在整个图像中滑动过滤器，你并不关心大象出现在哪里。 In practice, pooling also gives you invariance to translation, rotation and scaling, but more on that later.实践中，池化在转换、旋转和缩放时保持不变性。不过这些之后再说。 The second key aspect is (local) compositionality. Each filter composes a local patch of lower-level features into higher-level representation.第二个关键方面是局部组成性。每个过滤器将较低层次特征的本地补丁组成较高层次的表示。 That’s why CNNs are so powerful in Computer Vision. It makes intuitive sense that you build edges from pixels, shapes from edges, and more complex objects from shapes.这就是为什么CNN在计算机视觉领域如此强大。它直观的感觉到，你从像素构建边缘，从边缘构建形状，从形状构建更复杂的对象。 SO, HOW DOES ANY OF THIS APPLY TO NLP?那么，如何把这些应用到NLP中呢？Instead of image pixels, the input to most NLP tasks are sentences or documents represented as a matrix.不同于图像像素，大多数NLP任务的输入是被表示为矩阵的句子或文档 Each row of the matrix corresponds to one token, typically a word, but it could be a character. That is, each row is vector that represents a word.矩阵每一行对应一个Token，通常是一个字，但也可以是一个字符。也就是说每行是表示单词的一个向量。Typically, these vectors are word embeddings (low-dimensional representations) like word2vec or GloVe, but they could also be one-hot vectors that index the word into a vocabulary.通常，这些向量是字嵌入word embeddings（低维度表示），例如word2vec或者GloVe，但他们也可以是单词表示为词汇表索引的one-hot向量。 For a 10 word sentence using a 100-dimensional embedding we would have a 10×100 matrix as our input. That’s our “image”. 对于使用100维嵌入的一个10个单词的句子，我们得到10×100的矩阵作为我们的输入。这就是“图像”。 In vision, our filters slide over local patches of an image, but in NLP we typically use filters that slide over full rows of the matrix (words). 在视觉中，我们的过滤器在图像的局部区域上滑动，但在NLP中，我们通常使用在矩阵（字）的全部行上滑动的过滤器。 Thus, the “width” of our filters is usually the same as the width of the input matrix. The height, or region size, may vary, but sliding windows over 2-5 words at a time is typical. 因此，我们过滤器的宽度通常与输入矩阵的宽度相同。高度或者说区域大小，可以变化，但是通常每次2到5个词的滑动窗口是比较典型的。 Putting all the above together, a Convolutional Neural Network for NLP may look like this (take a few minutes and try understand this picture and how the dimensions are computed. You can ignore the pooling for now, we’ll explain that later): 综上所述，NLP的CNN可能看起来像这样（用几分钟来尝试理解这幅图片和维度是如何计算的，现下你可以忽略池化，我们过后解释） Illustration of a Convolutional Neural Network (CNN) architecture for sentence classification. 用于句子分类的CNN架构示例 Here we depict three filter region sizes: 2, 3 and 4, each of which has 2 filters. 这里描述了三种过滤器，区域大小分别是2，3，4。每个区域大小的有2个过滤器。 Every filter performs convolution on the sentence matrix and generates (variable-length) feature maps. 每个过滤器在句子矩阵上执行卷积运算，生成（长度可变的）特征映射 Then 1-max pooling is performed over each map, i.e., the largest number from each feature map is recorded. 之后对每个映射执行一个最大池化，即，记录来自每个特征映射的最大的值。 Thus a univariate feature vector is generated from all six maps, and these 6 features are concatenated to form a feature vector for the penultimate layer. 这样，从所有6个映射生成一个单变量特征向量，并且这6个特征级联构成倒数第二层的特征向量 The final softmax layer then receives this feature vector as input and uses it to classify the sentence; here we assume binary classification and hence depict two possible output states. 最后的softmax层接受这个特征向量作为输入，用它来进行句子分类。这里我们假定是二分类，因此描述了两种可能的输出状态。 Source: hang, Y., &amp; Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification What about the nice intuitions we had for Computer Vision? Location Invariance and local Compositionality made intuitive sense for images, but not so much for NLP. 我们对于CNN在计算机视觉方面的直觉，这里又是怎样呢？位置不变性和局部组成性在图像上起作用，但是在NLP上没起那么大作用。 You probably do care a lot where in the sentence a word appears. Pixels close to each other are likely to be semantically related (part of the same object), but the same isn’t always true for words. 你可能会关心单词在句子中出现的位置会带来什么影响。彼此靠近的像素可能是语义相关的（同一图像的一部分），但是对于字却并不总是如此。 In many languages, parts of phrases could be separated by several other words. The compositional aspect isn’t obvious either. 许多语言中，短语的部分可以被几个其他的单词分开。组成方面也并不明显。 Clearly, words compose in some ways, like an adjective modifying a noun, but how exactly this works what higher level representations actually “mean” isn’t as obvious as in the Computer Vision case. 很显然，词语通过几种方式构成，像形容词修饰名词。但这究竟是如何工作的、更高层次的表示实际上“意味”着什么，并不像在计算机视觉领域那么明显 Given all this, it seems like CNNs wouldn’t be a good fit for NLP tasks. 鉴于以上原因，CNNs似乎并不适合处理NLP任务 Recurrent Neural Networks make more intuitive sense.They resemble how we process language (or at least how we think we process language): Reading sequentially from left to right. 递归神经网络带来更加直观的感受。它类似于我们如何处理语言（或者至少我们认为的我们如何处理语言）：从左到右依次读。 Fortunately, this doesn’t mean that CNNs don’t work. 幸运的是，这并不意味着CNN不起作用。 All models are wrong, but some are useful. 所有的模型都是错的，但是有些是有用的。 It turns out that CNNs applied to NLP problems perform quite well. 事实证明，CNN用在NLP问题上，性能相当不错。 The simple Bag of Words model is an obvious oversimplification with incorrect assumptions, but has nonetheless been the standard approach for years and lead to pretty good results. 简单的词袋模型是一个带有错误假设的明显过度简化，但是仍然是多年的标准，还能有非常好的效果 A big argument for CNNs is that they are fast. Very fast. CNN的一大争论是它是相当快速的，十分快！ Convolutions are a central part of computer graphics and implemented on a hardware level on GPUs. 卷积是计算机图形的核心部分，并在GPU的硬件层级实现。 Compared to something like n-grams, CNNs are also efficient in terms of representation. 与N元模型之类的东西相比，CNN在表示方面也是高效的。 With a large vocabulary, computing anything more than 3-grams can quickly become expensive. Even Google doesn’t provide anything beyond 5-grams. 用大的词汇表，对于任何大于3元的表示，都会很快变得昂贵。即便是Google也没有提供5元之外的表示。 Convolutional Filters learn good representations automatically, without needing to represent the whole vocabulary. 卷积过滤器自动学习好的表示，不需要表示整个词汇表。 It’s completely reasonable to have filters of size larger than 5. 使用大小大于5的过滤器是完全合理的。 I like to think that many of the learned filters in the first layer are capturing features quite similar (but not limited) to n-grams, but represent them in a more compact way. 我倾向于认为第一层的许多学习到的过滤器捕获的特征与N元表示非常相似（但是不限于此）。但是以更紧凑的方式表示它们。 CNN HYPERPARAMETERSCNN的超参数Before explaining at how CNNs are applied to NLP tasks, let’s look at some of the choices you need to make when building a CNN.在介绍CNN如何用于NLP任务之前，先看下在构建CNN时需要做哪些选择。 Hopefully this will help you better understand the literature in the field.希望这将有助于你更好地了解这个领域的内容。 NARROW VS. WIDE CONVOLUTION窄卷积 VS 宽卷积When I explained convolutions above I neglected a little detail of how we apply the filter.在我前面解释卷积时，忽略了一个有关我们如何应用的过滤器的细节。 Applying a 3×3 filter at the center of the matrix works fine, but what about the edges?在矩阵的中心应用3×3过滤器可以正常工作，但边缘怎样处理呢？ How would you apply the filter to the first element of a matrix that doesn’t have any neighboring elements to the top and left?该如何将过滤器应用到矩阵的第一个元素呢？它的上边和左边可没有任何相邻的元素！ You can use zero-padding. All elements that would fall outside of the matrix are taken to be zero.您可以使用零填充。 将落在矩阵外的所有元素取为零。 By doing this you can apply the filter to every element of your input matrix, and get a larger or equally sized output.通过这样做，您可以对输入矩阵的每个元素应用过滤器，并获得更大或相等大小的输出。 Adding zero-padding is also called wide convolution, and not using zero-padding would be a narrow convolution.增加零填充也被称为宽卷积，不适用零填充是窄卷积。 An example in 1D looks like this: Narrow vs. Wide Convolution. Source: A Convolutional Neural Network for Modelling Sentences (2014)Narrow vs. Wide Convolution. Filter size 5, input size 7. Source: A Convolutional Neural Network for Modelling Sentences (2014) You can see how wide convolution is useful, or even necessary, when you have a large filter relative to the input size.您可以看到卷积相当有用，甚至是必要的，当你有一个相对于输入大小 比较大的过滤器的时候。 In the above, the narrow convolution yields an output of size (7-5) + 1=3, and a wide convolution an output of size (7+24 - 5) + 1 =11.More generally, the formula for the output size is n{out}=(n{in} + 2n{padding} - n{filter}) + 1 .上面的窄卷积产生的输出大小为：(7 - 5) + 1 = 3, 宽卷积输出大小为：(7 + 2 4 - 5) + 1 = 11更一般地，输出大小的公式为n {out} =（n {in} + 2 n {padding} -n {filter}）+1。 STRIDE SIZE步长Another hyperparameter for your convolutions is the stride size, defining by how much you want to shift your filter at each step.卷积的另一个超参数是步长大小，定义为你想在每一步移动过滤器多长。 In all the examples above the stride size was 1, and consecutive applications of the filter overlapped.在上述所有示例中，步幅大小为1，并且过滤器连续应用有重叠。 A larger stride size leads to fewer applications of the filter and a smaller output size.较大的步长导致过滤器的较少应用和较小的输出尺寸。 The following from the Stanford cs231 website shows stride sizes of 1 and 2 applied to a one-dimensional input:Convolution Stride Size. Left: Stride size 1. Right: Stride size 2. Source: http://cs231n.github.io/convolutional-networks/ In the literature we typically see stride sizes of 1, but a larger stride size may allow you to build a model that behaves somewhat similarly to a Recursive Neural Network, i.e. looks like a tree.在文献中，我们通常看到步幅大小为1，但是更大的步幅大小可能允许你建立一个行为有点类似于递归神经网络的模型，即看起来像一棵树。 POOLING LAYERS池化层A key aspect of Convolutional Neural Networks are pooling layers, typically applied after the convolutional layers.卷积神经网络的一个关键方面是池化层，通常在卷积层之后应用。 Pooling layers subsample their input. The most common way to do pooling it to apply a max operation to the result of each filter.池化层对其输入进行二次采样。 最常见的做法是将它应用于对每个过滤器的结果应用max操作。 You don’t necessarily need to pool over the complete matrix, you could also pool over a window.你不一定需要对完整的矩阵进行池化，你也可以用在一个窗口。 For example, the following shows max pooling for a 2×2 window (in NLP we typically are apply pooling over the complete output, yielding just a single number for each filter):Max pooling in CNN. Source: http://cs231n.github.io/convolutional-networks/#pool Why pooling? There are a couple of reasons.为什么要进行池化？有几种原因。 One property of pooling is that it provides a fixed size output matrix, which typically is required for classification.池化的一个特点是它提供了一个固定大小的输出矩阵，这通常是分类需要的。 For example, if you have 1,000 filters and you apply max pooling to each, you will get a 1000-dimensional output, regardless of the size of your filters, or the size of your input.例如，如果你有1000个过滤器，然后对每个过滤器应用最大池化，不管你的过滤器的大小或者是输入矩阵的大小，你都会得到一个1000维的输出。 This allows you to use variable size sentences, and variable size filters, but always get the same output dimensions to feed into a classifier.这允许你使用可变大小的句子和可变大小的过滤器，但总是能够获得相同的输出尺寸来用到分类器中。 Pooling also reduces the output dimensionality but (hopefully) keeps the most salient information.池化也降低了输出维数，但保持了最显著的信息（也是我们希望的）。 You can think of each filter as detecting a specific feature, such as detecting if the sentence contains a negation like “not amazing” for example.您可以将每个过滤器视为检测特定特征，例如检测句子是否包含诸如“不惊人”这样的否定。 If this phrase occurs somewhere in the sentence, the result of applying the filter to that region will yield a large value, but a small value in other regions.如果该短语出现在句子中的某个地方，则将过滤器应用于该区域的结果将产生大的值，但在其他区域中为小的值。 By performing the max operation you are keeping information about whether or not the feature appeared in the sentence, but you are losing information about where exactly it appeared.通过执行max操作，你保存了关于特征是否出现在句子中的信息，但你失去了关于它出现的位置的信息。 But isn’t this information about locality really useful? Yes, it is and it’s a bit similar to what a bag of n-grams model is doing.但是这个关于局部性的信息不是真的有用吗？ 是的，它是有点类似于N元模型做的。 You are losing global information about locality (where in a sentence something happens), but you are keeping local information captured by your filters, like “not amazing” being very different from “amazing not”.你失去了关于局部性的全局信息（在句子中发生了什么），但你保留了过滤器捕获的局部信息，例如“不惊人”与“惊人不”非常不同。 In imagine recognition, pooling also provides basic invariance to translating (shifting) and rotation.在图像识别中，池化还提供了对平移（移动）和旋转的基本不变性。 When you are pooling over a region, the output will stay approximately the same even if you shift/rotate the image by a few pixels, because the max operations will pick out the same value regardless.当您在区域上进行合并时，即使将图像移动/旋转几个像素，输出也将保持大致相同，因为最大操作将选择相同的值。 CHANNELS通道The last concept we need to understand are channels. Channels are different “views” of your input data.我们需要理解的最后一个概念是通道。 通道是输入数据的不同“视图”。 For example, in image recognition you typically have RGB (red, green, blue) channels.例如，在图像识别中，通常有RGB（红，绿，蓝）通道。 You can apply convolutions across channels, either with different or equal weights.您可以 以不同或相等的权重 在不同通道上应用卷积。 In NLP you could imagine having various channels as well: You could have a separate channels for different word embeddings (word2vec and GloVe for example), or you could have a channel for the same sentence represented in different languages, or phrased in different ways.在NLP中，你也可以想象有各种通道：你可以为不同的字嵌入（例如word2vec和GloVe）提供单独通道，或者你可以有一个通道用不同的语言表示相同的句子，或用不同的方式构成短语。 CONVOLUTIONAL NEURAL NETWORKS APPLIED TO NLP卷积神经网络在NLP的应用Let’s now look at some of the applications of CNNs to Natural Language Processing.现在让我们来看看CNN在自然语言处理中的一些应用。 I’ll try it summarize some of the research results. Invariably I’ll miss many interesting applications (do let me know in the comments), but I hope to cover at least some of the more popular results.我将尝试它总结一些研究结果。 总是我会错过许多有趣的应用程序（可以在评论留言通知我），但我希望至少覆盖一些更流行的结果。 The most natural fit for CNNs seem to be classifications tasks, such as Sentiment Analysis, Spam Detection or Topic Categorization.最自然的适合CNNs似乎是分类任务，如情感分析，垃圾邮件检测或主题分类。 Convolutions and pooling operations lose information about the local order of words, so that sequence tagging as in PoS Tagging or Entity Extraction is a bit harder to fit into a pure CNN architecture (though not impossible, you can add positional features to the input).卷积和池化操作丢失了关于字的局部顺序的信息，使得如在词性标记或实体提取中的序列标记有点难以适合于纯CNN架构（虽然不是不可能，可以向输入添加位置特征）。 [1] Evaluates a CNN architecture on various classification datasets, mostly comprised of Sentiment Analysis and Topic Categorization tasks.文献[1] 在各种分类数据集上评估CNN架构，主要由情感分析和主题分类任务组成。 The CNN architecture achieves very good performance across datasets, and new state-of-the-art on a few.CNN架构在不同数据集上实现了非常好的性能，并且在少数几个中实现了新的最先进的技术。 Surprisingly, the network used in this paper is quite simple, and that’s what makes it powerful.令人惊讶的是，文中使用的网络相当简单，这也正是它强大的原因。 The input layer is a sentence comprised of concatenated word2vec word embeddings.输入层是由连接的word2vec字嵌入组成的句子。 That’s followed by a convolutional layer with multiple filters, then a max-pooling layer, and finally a softmax classifier.接下来是一个具有多个过滤器的卷积层，然后是最大池层，最后是softmax分类器。 The paper also experiments with two different channels in the form of static and dynamic word embeddings, where one channel is adjusted during training and the other isn’t.该论文还 以静态和动态字嵌入的形式 对两个不同的通道进行实验，其中一个通道在训练期间调整，另一个训练期间不进行调整。 A similar, but somewhat more complex, architecture was previously proposed in [2].在参考文献[2]中提出了一个类似的，但稍微复杂的结构。 [6] Adds an additional layer that performs “semantic clustering” to this network architecture.参考文献[6]向这类网络架构中添加了一个额外的层，来执行“语义聚类”。 Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification [4] Trains a CNN from scratch, without the need for for pre-trained word vectors like word2vec or GloVe.参考文献[4] 从头开始训练了一个卷积神经网络，而不需要使用word2vec或GloVe这样的预先训练好的词向量。 It applies convolutions directly to one-hot vectors.它将卷积直接应用于one-hot向量。 The author also proposes a space-efficient bag-of-words-like representation for the input data, reducing the number of parameters the network needs to learn.作者还为输入数据提出了 一种节省空间的 类bag-of-words表示，减少了网络需要学习的参数数量。 In [5] the author extends the model with an additional unsupervised “region embedding” that is learned using a CNN predicting the context of text regions.在参考文献[5]中，作者扩展了 使用CNN预测文本区域的上下文学习 的额外的无监督“区域嵌入”的模型。 The approach in these papers seems to work well for long-form texts (like movie reviews), but their performance on short texts (like tweets) isn’t clear.这些文章中的方法似乎适用于长文本（如电影评论），但他们对短文本（如推文）的性能尚不明确。 Intuitively, it makes sense that using pre-trained word embeddings for short texts would yield larger gains than using them for long texts.直观上，对短文本使用预训练的字嵌入 将产生比用在长文本上 更大的增益。 Building a CNN architecture means that there are many hyperparameters to choose from, some of which I presented above: Input represenations (word2vec, GloVe, one-hot), number and sizes of convolution filters, pooling strategies (max, average), and activation functions (ReLU, tanh).构建CNN架构意味着有许多超参数可供选择，其中有一些我上面提到过：输入表示（word2vec，GloVe，one-hot），卷积过滤器的数量和大小，池策略（最大，平均）和 激活函数（ReLU，tanh）。 [7] performs an empirical evaluation on the effect of varying hyperparameters in CNN architectures, investigating their impact on performance and variance over multiple runs.参考文献[7] 对CNN架构中不同超参数的影响进行经验评估，调查它们对多次运行时的 性能和方差 的影响。 If you are looking to implement your own CNN for text classification, using the results of this paper as a starting point would be an excellent idea.如果你想实现你自己的CNN文本分类，使用本文的结果作为起点将是一个很好的主意。 A few results that stand out are that max-pooling always beat average pooling, that the ideal filter sizes are important but task-dependent, and that regularization doesn’t seem to make a big different in the NLP tasks that were considered.一些值得注意的结果是，max-pooling总是超过平均池，理想的过滤器大小是重要的，但是也看任务，并且正则化似乎并没有NLP任务中跟预想的有很大的不同。 A caveat of this research is that all the datasets were quite similar in terms of their document length, so the same guidelines may not apply to data that looks considerably different.这项研究的一个警告是，所有的数据集在其文档长度方面非常相似，因此相同的指南可能不适用于看起来相当不同的数据。 [8] explores CNNs for Relation Extraction and Relation Classification tasks.参考文献[8] 探讨CNN用在关系提取和关系分类任务。 In addition to the word vectors, the authors use the relative positions of words to the entities of interest as an input to the convolutional layer.除了词向量之外，作者使用 词对感兴趣实体的相对位置 作为卷积层的输入。 This models assumes that the positions of the entities are given, and that each example input contains one relation.这个模型假定实体的位置已经给出，并且每个输入样本包含一个关系。 [9] and [10] have explored similar models.参考文献[9]和[10] 探索了相似的模型。 Another interesting use case of CNNs in NLP can be found in [11] and [12], coming out of Microsoft Research.在NLP中使用CNN的另一个有趣的用例可以在[11]和[12]中找到，它们出自微软亚洲研究院。 These papers describe how to learn semantically meaningful representations of sentences that can be used for Information Retrieval.这些论文描述了如何学习语义有意义的句子表示，以供用于信息检索。 The example given in the papers includes recommending potentially interesting documents to users based on what they are currently reading.在论文中给出的示例包括 基于用户当前正在阅读的内容向他们推荐可能有趣的文档。 The sentence representations are trained based on search engine log data.这里面的句子表示是基于搜索引擎的日志数据来训练的。 Most CNN architectures learn embeddings (low-dimensional representations) for words and sentences in one way or another as part of their training procedure.大多数CNN架构学习嵌入（低维表示）的单词和句子用一种方式或另一种作为他们的训练过程的一部分。 Not all papers though focus on this aspect of training or investigate how meaningful the learned embeddings are.也不是所有的文章都关注在 训练或调查学习嵌入是如何有意义 这一个方面的。 [13] presents a CNN architecture to predict hashtags for Facebook posts, while at the same time generating meaningful embeddings for words and sentences.参考文献[13]提出一个CNN架构 来预测Facebook帖子的标签，同时为单词和句子生成有意义的嵌入。 These learned embeddings are then successfully applied to another task – recommending potentially interesting documents to users, trained based on clickstream data.这些学习的嵌入然后成功地应用到另一个任务 - 向用户推荐潜在有趣的文档，基于点击流数据训练。 CHARACTER-LEVEL CNNS字符级别的CNNSo far, all of the models presented were based on words. But there has also been research in applying CNNs directly to characters.到目前为止，所有的模型都是基于单词。 但是也有将CNN直接应用于字符的研究。 [14] learns character-level embeddings, joins them with pre-trained word embeddings, and uses a CNN for Part of Speech tagging.参考文献[14] 学习字符级嵌入，并与使用预训练的字嵌入连接起来，使用CNN进行词性标注。 [15][16] explores the use of CNNs to learn directly from characters, without the need for any pre-trained embeddings.参考文献 [15] [16]探索使用CNN直接从字符学习，而不需要任何预训练的嵌入。 Notably, the authors use a relatively deep network with a total of 9 layers, and apply it to Sentiment Analysis and Text Categorization tasks.值得注意的是，作者使用总共9层的相对深的网络，并将其应用于情感分析和文本分类任务。 Results show that learning directly from character-level input works very well on large datasets (millions of examples), but underperforms simpler models on smaller datasets (hundreds of thousands of examples).结果表明，直接从字符级输入学习 在大型数据集（数百万个示例）上工作得很好，但在较小的数据集（数十万个示例）上与简单模型相比 表现不佳。 [17] explores to application of character-level convolutions to Language Modeling, using the output of the character-level CNN as the input to an LSTM at each time step.参考文献[17] 探讨了将字符级卷积应用于语言建模，在每一步使用字符级CNN的输出作为LSTM的输入。 The same model is applied to various languages.将同样的模型应用于各种语言。 What’s amazing is that essentially all of the papers above were published in the past 1-2 years.令人惊讶的是，上述论文基本上都是在过去的1-2年内发表的。 Obviously there has been excellent work with CNNs on NLP before, as in Natural Language Processing (almost) from Scratch,显然，在NLP领域使用CNN在之前已经进行了优秀的工作，就像几乎从头开始进行自然语言处理 but the pace of new results and state of the art systems being published is clearly accelerating.但是新结果和公布出的最先进的系统的速度在明显加快。 PAPER REFERENCES[1] Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), 1746–1751.[2] Kalchbrenner, N., Grefenstette, E., &amp; Blunsom, P. (2014). A Convolutional Neural Network for Modelling Sentences. Acl, 655–665.[3] Santos, C. N. dos, &amp; Gatti, M. (2014). Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts. In COLING-2014 (pp. 69–78).[4] Johnson, R., &amp; Zhang, T. (2015). Effective Use of Word Order for Text Categorization with Convolutional Neural Networks. To Appear: NAACL-2015, (2011).[5] Johnson, R., &amp; Zhang, T. (2015). Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding.[6] Wang, P., Xu, J., Xu, B., Liu, C., Zhang, H., Wang, F., &amp; Hao, H. (2015). Semantic Clustering and Convolutional Neural Network for Short Text Categorization. Proceedings ACL 2015, 352–357.[7] Zhang, Y., &amp; Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification,[8] Nguyen, T. H., &amp; Grishman, R. (2015). Relation Extraction: Perspective from Convolutional Neural Networks. Workshop on Vector Modeling for NLP, 39–48.[9] Sun, Y., Lin, L., Tang, D., Yang, N., Ji, Z., &amp; Wang, X. (2015). Modeling Mention , Context and Entity with Neural Networks for Entity Disambiguation, (Ijcai), 1333–1339.[10] Zeng, D., Liu, K., Lai, S., Zhou, G., &amp; Zhao, J. (2014). Relation Classification via Convolutional Deep Neural Network. Coling, (2011), 2335–2344.[11] Gao, J., Pantel, P., Gamon, M., He, X., &amp; Deng, L. (2014). Modeling Interestingness with Deep Neural Networks.[12] Shen, Y., He, X., Gao, J., Deng, L., &amp; Mesnil, G. (2014). A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval. Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management – CIKM ’14, 101–110.[13] Weston, J., &amp; Adams, K. (2014). # T AG S PACE : Semantic Embeddings from Hashtags, 1822–1827.[14] Santos, C., &amp; Zadrozny, B. (2014). Learning Character-level Representations for Part-of-Speech Tagging. Proceedings of the 31st International Conference on Machine Learning, ICML-14(2011), 1818–1826.[15] Zhang, X., Zhao, J., &amp; LeCun, Y. (2015). Character-level Convolutional Networks for Text Classification, 1–9.[16] Zhang, X., &amp; LeCun, Y. (2015). Text Understanding from Scratch. arXiv E-Prints, 3, 011102.[17] Kim, Y., Jernite, Y., Sontag, D., &amp; Rush, A. M. (2015). Character-Aware Neural Language Models.","tags":[]},{"title":"Linux服务器工具","date":"2016-08-17T09:38:56.000Z","path":"2016/08/17/2016-08-17-linuxfu-wu-qi-gong-ju/","text":"集群间拷贝在集群环境下，经常会遇到需要从一台服务器上向多个服务器拷贝软件包或者配置的情况，这时，pscp就派上用场了 基本使用命令pscp -h hosts_file local_source_file remote_out_dir 其中的hosts_file内容如下 host1:22 host2:22 ... 其他参数指定用户，使用 -l username 参数传递目录，添加 -r 参数","tags":[]},{"title":"python函数","date":"2016-08-16T16:29:39.000Z","path":"2016/08/17/2016-08-17-pythonhan-shu/","text":"常用Python函数介绍 字符串函数","tags":[]},{"title":"语言模型","date":"2016-08-16T13:59:51.000Z","path":"2016/08/16/2016-08-16-yu-yan-mo-xing/","text":"语言模型val idx = List(1, 2, 3, 4) 上面是一个scala的常量定义 测试公式$$f’\\left( x\\right) = \\lim _{x\\rightarrow 0}\\dfrac {f\\left( x+\\Delta x\\right) - f\\left( x\\right)}{\\Delta x}$$","tags":[]}]